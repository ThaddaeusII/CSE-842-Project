{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shmuhammadd/semantic_relatedness/blob/main/Simple_English_Baseline_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewwVGDM3IyXY"
   },
   "source": [
    "# Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T6myajMlIyXZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/b/School/CSE842/Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr, linregress\n",
    "import Levenshtein\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gn_ikaypIyXZ"
   },
   "source": [
    "# Data Import / Format / Export\n",
    "\n",
    "Functions for importing, formatting, and exporting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1PhipxbhIyXa",
    "outputId": "1da7308c-a799-4fe0-897e-3e4a23ea15c2"
   },
   "outputs": [],
   "source": [
    "# Load data from csv, format into proper split\n",
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Split_Text'] = data['Text'].apply(lambda x: x.split(\"\\n\"))\n",
    "    data['Pred_Score'] = 0.0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(s1, s2):\n",
    "    set1, set2 = set(s1), set(s2)\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_overlap(s1, s2):\n",
    "    set1, set2 = set(s1), set(s2)\n",
    "    return len(set1.intersection(set2)) / len(set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional features added to RoBERTa embeddings\n",
    "def compute_custom_metrics(row):\n",
    "    metrics = {}\n",
    "    cosine_sim = F.cosine_similarity(row[\"Embedding1\"].unsqueeze(0), row[\"Embedding2\"].unsqueeze(0))\n",
    "    metrics[\"Cosine_Similarity\"] = cosine_sim.item()\n",
    "\n",
    "    set1 = set(row[\"Sentence1\"].split())\n",
    "    set2 = set(row[\"Sentence2\"].split())\n",
    "    jaccard_sim = len(set1.intersection(set2)) / len(set1.union(set2)) if len(set1.union(set2)) > 0 else 0\n",
    "    metrics[\"Jaccard_Similarity\"] = jaccard_sim\n",
    "\n",
    "    metrics[\"Length_Diff\"] = abs(len(row[\"Sentence1\"].split()) - len(row[\"Sentence2\"].split()))\n",
    "\n",
    "    metrics['Levenshtein_Distance'] = Levenshtein.distance(row['Sentence1'], row['Sentence2'])\n",
    "\n",
    "    word_overlap_score = word_overlap(row[\"Sentence1\"].split(), row[\"Sentence2\"].split())\n",
    "    metrics['Word_Overlap'] = word_overlap_score\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 10:08:07.845405: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-26 10:08:08.486743: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-26 10:08:09.006321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732633689.432969    7193 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732633689.550283    7193 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-26 10:08:10.693778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed batch sizes due to memory issues\n",
    "def get_roberta_embeddings(sentences, batch_size=32):\n",
    "    embeddings_list = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings_list.append(embeddings)\n",
    "    return torch.cat(embeddings_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_roberta(data, batch_size=32):\n",
    "    # Split into two sentences\n",
    "    data[['Sentence1', 'Sentence2']] = pd.DataFrame(data['Split_Text'].tolist(), index=data.index)\n",
    "    \n",
    "    # Lowercase sentences, strip whitespace\n",
    "    data[\"Sentence1\"] = data[\"Sentence1\"].str.lower().str.strip()\n",
    "    data[\"Sentence2\"] = data[\"Sentence2\"].str.lower().str.strip()\n",
    "\n",
    "    # Generate RoBERTa embeddings in batches (keeping everything as tensors)\n",
    "    embeddings1 = get_roberta_embeddings(data[\"Sentence1\"].tolist(), batch_size)\n",
    "    embeddings2 = get_roberta_embeddings(data[\"Sentence2\"].tolist(), batch_size)\n",
    "    \n",
    "    # Save embeddings for custom metrics\n",
    "    data[\"Embedding1\"] = list(embeddings1)\n",
    "    data[\"Embedding2\"] = list(embeddings2)\n",
    "\n",
    "    # Compute custom metrics for each row\n",
    "    metrics = data.apply(compute_custom_metrics, axis=1, result_type=\"expand\")\n",
    "\n",
    "    # Convert metrics to tensor\n",
    "    metrics_tensor = torch.tensor(metrics.values, dtype=torch.float32)\n",
    "\n",
    "    # Standardize custom metrics\n",
    "    scaler = StandardScaler()\n",
    "    standardized_metrics = scaler.fit_transform(metrics_tensor.numpy())\n",
    "    standardized_metrics_tensor = torch.tensor(standardized_metrics, dtype=torch.float32)\n",
    "\n",
    "    # Combine embeddings and metrics\n",
    "    features = torch.cat([\n",
    "        embeddings1,\n",
    "        embeddings2,\n",
    "        standardized_metrics_tensor\n",
    "    ], dim=1)\n",
    "\n",
    "    # Returned processed features as tensors\n",
    "    return features, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Irz_4Wm6IyXb"
   },
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, scores):\n",
    "    pearson_corr, _ = pearsonr(scores, preds)\n",
    "    spearman_corr, _ = spearmanr(scores, preds)\n",
    "    _, _, r, _, _ = linregress(scores, preds) # probably a better way of doing this, fix later\n",
    "    r2 = r**2\n",
    "    mse = ((scores - preds)**2).mean() # Scikit's mean_squared_error complained about being deprecated, so this is my temp fix\n",
    "    return (pearson_corr, spearman_corr, r2, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure these match the metrics above\n",
    "def display_metrics(metrics, title=\"Metrics:\"):\n",
    "    print(title)\n",
    "    print(\"Pearson Corr:\", metrics[0])\n",
    "    print(\"Spearman Corr:\", metrics[1])\n",
    "    print(\"R^2:\", metrics[2])\n",
    "    print(\"MSE:\", metrics[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>Split_Text</th>\n",
       "      <th>Pred_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG-train-0000</td>\n",
       "      <td>It that happens, just pull the plug.\\nif that ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[It that happens, just pull the plug., if that...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG-train-0001</td>\n",
       "      <td>A black dog running through water.\\nA black do...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[A black dog running through water., A black d...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG-train-0002</td>\n",
       "      <td>I've been searchingthe entire abbey for you.\\n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I've been searchingthe entire abbey for you.,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG-train-0003</td>\n",
       "      <td>If he is good looking and has a good personali...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[If he is good looking and has a good personal...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENG-train-0004</td>\n",
       "      <td>She does not hate you, she is just annoyed wit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[She does not hate you, she is just annoyed wi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PairID                                               Text  Score  \\\n",
       "0  ENG-train-0000  It that happens, just pull the plug.\\nif that ...    1.0   \n",
       "1  ENG-train-0001  A black dog running through water.\\nA black do...    1.0   \n",
       "2  ENG-train-0002  I've been searchingthe entire abbey for you.\\n...    1.0   \n",
       "3  ENG-train-0003  If he is good looking and has a good personali...    1.0   \n",
       "4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0   \n",
       "\n",
       "                                          Split_Text  Pred_Score  \n",
       "0  [It that happens, just pull the plug., if that...         0.0  \n",
       "1  [A black dog running through water., A black d...         0.0  \n",
       "2  [I've been searchingthe entire abbey for you.,...         0.0  \n",
       "3  [If he is good looking and has a good personal...         0.0  \n",
       "4  [She does not hate you, she is just annoyed wi...         0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_data(\"./Semantic_Relatedness_SemEval2024/Track A/eng/eng_train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>Split_Text</th>\n",
       "      <th>Pred_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG-test-0000</td>\n",
       "      <td>Egypt's Brotherhood stands ground after killin...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[Egypt's Brotherhood stands ground after killi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG-test-0001</td>\n",
       "      <td>install it for fre and get to know what all u ...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>[install it for fre and get to know what all u...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG-test-0002</td>\n",
       "      <td>Also, it was one of the debut novels that I wa...</td>\n",
       "      <td>0.49</td>\n",
       "      <td>[Also, it was one of the debut novels that I w...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG-test-0003</td>\n",
       "      <td>Therefore, you can use the code BRAIL, BASIL, ...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>[Therefore, you can use the code BRAIL, BASIL,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENG-test-0004</td>\n",
       "      <td>Solid YA novel with a funky take on zombies an...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>[Solid YA novel with a funky take on zombies a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PairID                                               Text  Score  \\\n",
       "0  ENG-test-0000  Egypt's Brotherhood stands ground after killin...   0.70   \n",
       "1  ENG-test-0001  install it for fre and get to know what all u ...   0.71   \n",
       "2  ENG-test-0002  Also, it was one of the debut novels that I wa...   0.49   \n",
       "3  ENG-test-0003  Therefore, you can use the code BRAIL, BASIL, ...   0.27   \n",
       "4  ENG-test-0004  Solid YA novel with a funky take on zombies an...   0.32   \n",
       "\n",
       "                                          Split_Text  Pred_Score  \n",
       "0  [Egypt's Brotherhood stands ground after killi...         0.0  \n",
       "1  [install it for fre and get to know what all u...         0.0  \n",
       "2  [Also, it was one of the debut novels that I w...         0.0  \n",
       "3  [Therefore, you can use the code BRAIL, BASIL,...         0.0  \n",
       "4  [Solid YA novel with a funky take on zombies a...         0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_data(\"./Semantic_Relatedness_SemEval2024/Track A/eng/eng_test_with_labels.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5500, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>Split_Text</th>\n",
       "      <th>Pred_Score</th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>Embedding1</th>\n",
       "      <th>Embedding2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG-train-0000</td>\n",
       "      <td>It that happens, just pull the plug.\\nif that ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[It that happens, just pull the plug., if that...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>it that happens, just pull the plug.</td>\n",
       "      <td>if that ever happens, just pull the plug.</td>\n",
       "      <td>[tensor(-0.1094), tensor(0.1345), tensor(-0.04...</td>\n",
       "      <td>[tensor(-0.1166), tensor(0.1211), tensor(-0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG-train-0001</td>\n",
       "      <td>A black dog running through water.\\nA black do...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[A black dog running through water., A black d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a black dog running through water.</td>\n",
       "      <td>a black dog is running through some water.</td>\n",
       "      <td>[tensor(-0.1038), tensor(0.0925), tensor(-0.00...</td>\n",
       "      <td>[tensor(-0.0920), tensor(0.0753), tensor(-0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG-train-0002</td>\n",
       "      <td>I've been searchingthe entire abbey for you.\\n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I've been searchingthe entire abbey for you.,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i've been searchingthe entire abbey for you.</td>\n",
       "      <td>i'm looking for you all over the abbey.</td>\n",
       "      <td>[tensor(-0.1287), tensor(0.0527), tensor(-0.01...</td>\n",
       "      <td>[tensor(-0.1227), tensor(0.0650), tensor(0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG-train-0003</td>\n",
       "      <td>If he is good looking and has a good personali...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[If he is good looking and has a good personal...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>if he is good looking and has a good personali...</td>\n",
       "      <td>if he's good looking, and a good personality, ...</td>\n",
       "      <td>[tensor(-0.0881), tensor(0.0848), tensor(-0.01...</td>\n",
       "      <td>[tensor(-0.1034), tensor(0.0648), tensor(-0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENG-train-0004</td>\n",
       "      <td>She does not hate you, she is just annoyed wit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[She does not hate you, she is just annoyed wi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>she does not hate you, she is just annoyed wit...</td>\n",
       "      <td>she doesn't hate you, she is just annoyed.</td>\n",
       "      <td>[tensor(-0.0909), tensor(0.1275), tensor(0.007...</td>\n",
       "      <td>[tensor(-0.1069), tensor(0.1247), tensor(0.013...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PairID                                               Text  Score  \\\n",
       "0  ENG-train-0000  It that happens, just pull the plug.\\nif that ...    1.0   \n",
       "1  ENG-train-0001  A black dog running through water.\\nA black do...    1.0   \n",
       "2  ENG-train-0002  I've been searchingthe entire abbey for you.\\n...    1.0   \n",
       "3  ENG-train-0003  If he is good looking and has a good personali...    1.0   \n",
       "4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0   \n",
       "\n",
       "                                          Split_Text  Pred_Score  \\\n",
       "0  [It that happens, just pull the plug., if that...         0.0   \n",
       "1  [A black dog running through water., A black d...         0.0   \n",
       "2  [I've been searchingthe entire abbey for you.,...         0.0   \n",
       "3  [If he is good looking and has a good personal...         0.0   \n",
       "4  [She does not hate you, she is just annoyed wi...         0.0   \n",
       "\n",
       "                                           Sentence1  \\\n",
       "0               it that happens, just pull the plug.   \n",
       "1                 a black dog running through water.   \n",
       "2       i've been searchingthe entire abbey for you.   \n",
       "3  if he is good looking and has a good personali...   \n",
       "4  she does not hate you, she is just annoyed wit...   \n",
       "\n",
       "                                           Sentence2  \\\n",
       "0          if that ever happens, just pull the plug.   \n",
       "1         a black dog is running through some water.   \n",
       "2            i'm looking for you all over the abbey.   \n",
       "3  if he's good looking, and a good personality, ...   \n",
       "4         she doesn't hate you, she is just annoyed.   \n",
       "\n",
       "                                          Embedding1  \\\n",
       "0  [tensor(-0.1094), tensor(0.1345), tensor(-0.04...   \n",
       "1  [tensor(-0.1038), tensor(0.0925), tensor(-0.00...   \n",
       "2  [tensor(-0.1287), tensor(0.0527), tensor(-0.01...   \n",
       "3  [tensor(-0.0881), tensor(0.0848), tensor(-0.01...   \n",
       "4  [tensor(-0.0909), tensor(0.1275), tensor(0.007...   \n",
       "\n",
       "                                          Embedding2  \n",
       "0  [tensor(-0.1166), tensor(0.1211), tensor(-0.04...  \n",
       "1  [tensor(-0.0920), tensor(0.0753), tensor(-0.00...  \n",
       "2  [tensor(-0.1227), tensor(0.0650), tensor(0.013...  \n",
       "3  [tensor(-0.1034), tensor(0.0648), tensor(-0.02...  \n",
       "4  [tensor(-0.1069), tensor(0.1247), tensor(0.013...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_data = preprocess_with_roberta(train_data)\n",
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5500, 1541])\n",
      "tensor([[-1.0935e-01,  1.3450e-01, -4.0140e-02,  ..., -6.6621e-01,\n",
      "         -1.7812e+00,  3.0215e+00],\n",
      "        [-1.0378e-01,  9.2506e-02, -3.3523e-03,  ..., -3.3545e-01,\n",
      "         -1.7041e+00,  3.7620e+00],\n",
      "        [-1.2873e-01,  5.2712e-02, -1.4779e-02,  ..., -6.6621e-01,\n",
      "         -7.4082e-01, -6.8096e-01],\n",
      "        ...,\n",
      "        [-1.2664e-01,  6.4262e-02,  4.8498e-03,  ...,  1.6491e+00,\n",
      "          4.5370e-01, -8.4551e-01],\n",
      "        [-9.2601e-02,  1.0219e-01,  2.0890e-03,  ..., -6.6621e-01,\n",
      "         -5.8669e-01, -9.0311e-01],\n",
      "        [-9.2274e-02,  1.2332e-01, -5.6928e-03,  ..., -3.3545e-01,\n",
      "         -5.4815e-01, -7.7821e-03]])\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_features)\n",
    "torch.save(train_features, \"train_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5500])\n",
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "train_labels = torch.tensor(train_data['Score'], dtype=torch.float32)\n",
    "print(train_labels.shape)\n",
    "print(train_labels)\n",
    "torch.save(train_labels, \"train_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2600, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>Split_Text</th>\n",
       "      <th>Pred_Score</th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>Embedding1</th>\n",
       "      <th>Embedding2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG-test-0000</td>\n",
       "      <td>Egypt's Brotherhood stands ground after killin...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[Egypt's Brotherhood stands ground after killi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>egypt's brotherhood stands ground after killings</td>\n",
       "      <td>egypt: muslim brotherhood stands behind morsi</td>\n",
       "      <td>[tensor(-0.0477), tensor(0.0616), tensor(0.005...</td>\n",
       "      <td>[tensor(-0.0468), tensor(0.0575), tensor(0.008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG-test-0001</td>\n",
       "      <td>install it for fre and get to know what all u ...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>[install it for fre and get to know what all u...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>install it for fre and get to know what all u ...</td>\n",
       "      <td>install the program, which is free to download...</td>\n",
       "      <td>[tensor(-0.0327), tensor(0.0544), tensor(-0.03...</td>\n",
       "      <td>[tensor(-0.0901), tensor(0.1307), tensor(-0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG-test-0002</td>\n",
       "      <td>Also, it was one of the debut novels that I wa...</td>\n",
       "      <td>0.49</td>\n",
       "      <td>[Also, it was one of the debut novels that I w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>also, it was one of the debut novels that i wa...</td>\n",
       "      <td>pretty much the first thing people mentioned w...</td>\n",
       "      <td>[tensor(-0.1055), tensor(0.0924), tensor(0.001...</td>\n",
       "      <td>[tensor(-0.1187), tensor(0.0733), tensor(-0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG-test-0003</td>\n",
       "      <td>Therefore, you can use the code BRAIL, BASIL, ...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>[Therefore, you can use the code BRAIL, BASIL,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>therefore, you can use the code brail, basil, ...</td>\n",
       "      <td>you can watch the wiggles every day on nick jr.</td>\n",
       "      <td>[tensor(-0.0907), tensor(0.1206), tensor(-0.03...</td>\n",
       "      <td>[tensor(-0.0965), tensor(0.0422), tensor(0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENG-test-0004</td>\n",
       "      <td>Solid YA novel with a funky take on zombies an...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>[Solid YA novel with a funky take on zombies a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>solid ya novel with a funky take on zombies an...</td>\n",
       "      <td>my 13-year-old son recommended this book to me...</td>\n",
       "      <td>[tensor(-0.1368), tensor(0.0987), tensor(-0.00...</td>\n",
       "      <td>[tensor(-0.0597), tensor(0.0636), tensor(-0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PairID                                               Text  Score  \\\n",
       "0  ENG-test-0000  Egypt's Brotherhood stands ground after killin...   0.70   \n",
       "1  ENG-test-0001  install it for fre and get to know what all u ...   0.71   \n",
       "2  ENG-test-0002  Also, it was one of the debut novels that I wa...   0.49   \n",
       "3  ENG-test-0003  Therefore, you can use the code BRAIL, BASIL, ...   0.27   \n",
       "4  ENG-test-0004  Solid YA novel with a funky take on zombies an...   0.32   \n",
       "\n",
       "                                          Split_Text  Pred_Score  \\\n",
       "0  [Egypt's Brotherhood stands ground after killi...         0.0   \n",
       "1  [install it for fre and get to know what all u...         0.0   \n",
       "2  [Also, it was one of the debut novels that I w...         0.0   \n",
       "3  [Therefore, you can use the code BRAIL, BASIL,...         0.0   \n",
       "4  [Solid YA novel with a funky take on zombies a...         0.0   \n",
       "\n",
       "                                           Sentence1  \\\n",
       "0   egypt's brotherhood stands ground after killings   \n",
       "1  install it for fre and get to know what all u ...   \n",
       "2  also, it was one of the debut novels that i wa...   \n",
       "3  therefore, you can use the code brail, basil, ...   \n",
       "4  solid ya novel with a funky take on zombies an...   \n",
       "\n",
       "                                           Sentence2  \\\n",
       "0      egypt: muslim brotherhood stands behind morsi   \n",
       "1  install the program, which is free to download...   \n",
       "2  pretty much the first thing people mentioned w...   \n",
       "3    you can watch the wiggles every day on nick jr.   \n",
       "4  my 13-year-old son recommended this book to me...   \n",
       "\n",
       "                                          Embedding1  \\\n",
       "0  [tensor(-0.0477), tensor(0.0616), tensor(0.005...   \n",
       "1  [tensor(-0.0327), tensor(0.0544), tensor(-0.03...   \n",
       "2  [tensor(-0.1055), tensor(0.0924), tensor(0.001...   \n",
       "3  [tensor(-0.0907), tensor(0.1206), tensor(-0.03...   \n",
       "4  [tensor(-0.1368), tensor(0.0987), tensor(-0.00...   \n",
       "\n",
       "                                          Embedding2  \n",
       "0  [tensor(-0.0468), tensor(0.0575), tensor(0.008...  \n",
       "1  [tensor(-0.0901), tensor(0.1307), tensor(-0.04...  \n",
       "2  [tensor(-0.1187), tensor(0.0733), tensor(-0.01...  \n",
       "3  [tensor(-0.0965), tensor(0.0422), tensor(0.012...  \n",
       "4  [tensor(-0.0597), tensor(0.0636), tensor(-0.06...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features, test_data = preprocess_with_roberta(test_data)\n",
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2600, 1541])\n",
      "tensor([[-4.7668e-02,  6.1642e-02,  5.0457e-03,  ..., -1.0263e+00,\n",
      "         -8.5190e-01, -1.4553e-01],\n",
      "        [-3.2685e-02,  5.4365e-02, -3.8078e-02,  ..., -9.9136e-02,\n",
      "          4.3447e-01,  3.7257e-01],\n",
      "        [-1.0550e-01,  9.2436e-02,  1.2523e-03,  ..., -7.1725e-01,\n",
      "          6.6836e-01, -9.1528e-01],\n",
      "        ...,\n",
      "        [-1.4587e-01,  1.0338e-01,  1.2481e-02,  ..., -4.0819e-01,\n",
      "         -6.1801e-01, -1.4926e+00],\n",
      "        [-1.1664e-01,  8.5891e-02,  1.1200e-02,  ..., -4.0819e-01,\n",
      "         -1.1126e-01, -1.4926e+00],\n",
      "        [-1.0706e-01,  9.1530e-02,  1.7318e-03,  ...,  1.1371e+00,\n",
      "          8.6326e-01, -7.7944e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(test_features.shape)\n",
    "print(test_features)\n",
    "torch.save(test_features, \"test_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2600])\n",
      "tensor([0.7000, 0.7100, 0.4900,  ..., 0.4500, 0.4500, 0.2200])\n"
     ]
    }
   ],
   "source": [
    "test_labels = torch.tensor(test_data['Score'], dtype=torch.float32)\n",
    "print(test_labels.shape)\n",
    "print(test_labels)\n",
    "torch.save(test_labels, \"test_labels.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([5500, 1541])\n",
      "y_train: torch.Size([5500])\n",
      "X_test: torch.Size([2600, 1541])\n",
      "y_test: torch.Size([2600])\n"
     ]
    }
   ],
   "source": [
    "# Update data for torch usage\n",
    "X_train = train_features\n",
    "y_train = train_labels\n",
    "X_test = test_features\n",
    "y_test = test_labels\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_args = TrainingArguments(\\n    output_dir=\\'./results\\',\\n    num_train_epochs=3,\\n    per_device_train_batch_size=16,\\n    per_device_eval_batch_size=16,\\n    warmup_steps=500,\\n    weight_decay=0.01,\\n    logging_dir=\\'./logs\\',\\n    logging_steps=10,\\n    evaluation_strategy=\"epoch\",\\n)'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_data = torch.utils.data.TensorDataset(X_train, y_train)\\ntest_data = torch.utils.data.TensorDataset(X_test, y_test)'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_data = torch.utils.data.TensorDataset(X_test, y_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_data,\\n    eval_dataset=train_data,\\n    data_collator=collate_fn,\\n)'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=train_data,\n",
    "    data_collator=collate_fn,\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainer.train()'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''trainer.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed assistance with getting a differentiable spearman correlation for loss function\n",
    "# https://forum.numer.ai/t/differentiable-spearman-in-pytorch-optimize-for-corr-directly/2287/26\n",
    "import torchsort\n",
    "\n",
    "def corrcoef(target, pred):\n",
    "    pred_n = pred - pred.mean()\n",
    "    target_n = target - target.mean()\n",
    "    pred_n = pred_n / pred_n.norm()\n",
    "    target_n = target_n / target_n.norm()\n",
    "    return (pred_n * target_n).sum()\n",
    "\n",
    "def spearman_loss(pred, target, x=1e-2):\n",
    "    pred = torchsort.soft_rank(pred.reshape(1,-1),regularization_strength=x)\n",
    "    target = torchsort.soft_rank(target.reshape(1,-1),regularization_strength=x)\n",
    "    pred = pred - pred.mean()\n",
    "    pred = pred / pred.norm()\n",
    "    target = target - target.mean()\n",
    "    target = target / target.norm()\n",
    "    return 1 - (pred * target).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size=1539, hidden_size=128, num_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "    \n",
    "    def split(self, X, y, s = 0.8):\n",
    "        dataset = TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
    "                        torch.tensor(y, dtype=torch.float32))\n",
    "        train_size = int(s * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        X_train, y_train = zip(*train_dataset)\n",
    "        X_train = torch.stack(X_train)\n",
    "        y_train = torch.stack(y_train)\n",
    "\n",
    "        X_val, y_val = zip(*val_dataset)\n",
    "        X_val = torch.stack(X_val)\n",
    "        y_val = torch.stack(y_val)\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=20, lr=1e-4, weight_decay=1e-4):\n",
    "        X_train, y_train, X_val, y_val = self.split(X, y)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self(X_train)\n",
    "            loss = spearman_loss(y_train, y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = self(X_val)\n",
    "                val_loss = spearman_loss(y_val, val_pred).item()\n",
    "\n",
    "            if epoch % (num_epochs // 10) == 0 or epoch == num_epochs - 1:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Spearman Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.state_dict(), './best_model.pth')\n",
    "                \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformation, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "        self.shift = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(-1)\n",
    "        return x * self.scale + self.shift\n",
    "    \n",
    "    def fit(self, X_train, y_train, num_epochs=1000, lr=0.01):\n",
    "        optimizer = optim.Adam(self.parameters(), lr)\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self(X_train)\n",
    "            loss = nn.functional.mse_loss(y_train.squeeze(), y_pred.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % (num_epochs // 10) == 0 or epoch == num_epochs - 1:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, MSE Loss: {loss.item():.4f}\")\n",
    "                \n",
    "    def transform(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7193/2816189488.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
      "/tmp/ipykernel_7193/2816189488.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Spearman Loss: 1.0322, Val Loss: 0.8067\n",
      "Epoch 21/200, Spearman Loss: 0.3991, Val Loss: 0.4064\n",
      "Epoch 41/200, Spearman Loss: 0.3712, Val Loss: 0.3873\n",
      "Epoch 61/200, Spearman Loss: 0.3297, Val Loss: 0.3417\n",
      "Epoch 81/200, Spearman Loss: 0.3729, Val Loss: 0.4028\n",
      "Epoch 101/200, Spearman Loss: 0.2727, Val Loss: 0.2863\n",
      "Epoch 121/200, Spearman Loss: 0.2634, Val Loss: 0.2935\n",
      "Epoch 141/200, Spearman Loss: 0.2497, Val Loss: 0.2892\n",
      "Epoch 161/200, Spearman Loss: 0.2434, Val Loss: 0.3006\n",
      "Epoch 181/200, Spearman Loss: 0.2340, Val Loss: 0.3043\n",
      "Epoch 200/200, Spearman Loss: 0.3189, Val Loss: 0.3181\n",
      "\n",
      "Epoch 1/1000, MSE Loss: 57.3320\n",
      "Epoch 101/1000, MSE Loss: 2.7248\n",
      "Epoch 201/1000, MSE Loss: 0.0350\n",
      "Epoch 301/1000, MSE Loss: 0.0237\n",
      "Epoch 401/1000, MSE Loss: 0.0237\n",
      "Epoch 501/1000, MSE Loss: 0.0237\n",
      "Epoch 601/1000, MSE Loss: 0.0237\n",
      "Epoch 701/1000, MSE Loss: 0.0237\n",
      "Epoch 801/1000, MSE Loss: 0.0237\n",
      "Epoch 901/1000, MSE Loss: 0.0237\n",
      "Epoch 1000/1000, MSE Loss: 0.0237\n"
     ]
    }
   ],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "model = Model(input_size=X_train.shape[1], hidden_size=128, num_layers=2)\n",
    "model.fit(X, y, 200, 0.1, 0.0001)\n",
    "\n",
    "raw_pred = model.predict(X)\n",
    "print()\n",
    "\n",
    "trans = Transformation()\n",
    "trans.fit(raw_pred, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Pearson Corr: 0.7178010428554639\n",
      "Spearman Corr: 0.7107450570732378\n",
      "R^2: 0.515238297716144\n",
      "MSE: 57.331993\n",
      "\n",
      "Testing Metrics:\n",
      "Pearson Corr: 0.650061010887935\n",
      "Spearman Corr: 0.6765825204764284\n",
      "R^2: 0.42257923215071114\n",
      "MSE: 56.384674\n"
     ]
    }
   ],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "train_preds_np = np.array(train_preds).flatten()\n",
    "train_y_np = np.array(y_train).flatten()\n",
    "test_preds_np = np.array(test_preds).flatten()\n",
    "test_y_np = np.array(y_test).flatten()\n",
    "\n",
    "train_metrics = calculate_metrics(train_y_np, train_preds_np)\n",
    "test_metrics = calculate_metrics(test_y_np, test_preds_np)\n",
    "\n",
    "display_metrics(train_metrics, \"Training Metrics:\")\n",
    "print()\n",
    "display_metrics(test_metrics, \"Testing Metrics:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred vs True for training data\n",
      "19.1608, 1.0000\n",
      "16.1683, 1.0000\n",
      "6.4047, 1.0000\n",
      "15.1955, 1.0000\n",
      "16.1668, 1.0000\n",
      "20.0277, 1.0000\n",
      "16.6414, 1.0000\n",
      "16.7724, 1.0000\n",
      "16.7846, 1.0000\n",
      "18.2593, 1.0000\n",
      "\n",
      "Pred vs True for testing data\n",
      "-0.3047, 0.7000\n",
      "-1.5987, 0.7100\n",
      "1.1716, 0.4900\n",
      "-5.7783, 0.2700\n",
      "-6.8394, 0.3200\n",
      "0.5209, 0.4300\n",
      "-6.8074, 0.3100\n",
      "-5.9296, 0.3200\n",
      "5.7925, 0.7700\n",
      "-6.3460, 0.3400\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred vs True for training data\")\n",
    "for i in range(10):\n",
    "    print(f\"{train_preds_np[i]:.4f}, {train_labels[i]:.4f}\")\n",
    "print()\n",
    "print(\"Pred vs True for testing data\")\n",
    "for i in range(10):\n",
    "    print(f\"{test_preds_np[i]:.4f}, {test_labels[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Pearson Corr: 0.7178010318765529\n",
      "Spearman Corr: 0.710745149944463\n",
      "R^2: 0.5152382992135832\n",
      "MSE: 0.023741225\n",
      "\n",
      "Testing Metrics:\n",
      "Pearson Corr: 0.6500610125665887\n",
      "Spearman Corr: 0.6765823541113402\n",
      "R^2: 0.42257923137626024\n",
      "MSE: 0.020314747\n"
     ]
    }
   ],
   "source": [
    "train_preds_trans = trans.transform(train_preds)\n",
    "test_preds_trans = trans.transform(test_preds)\n",
    "\n",
    "train_preds_trans_np = np.array(train_preds_trans).flatten()\n",
    "train_y_trans_np = np.array(y_train).flatten()\n",
    "test_preds_trans_np = np.array(test_preds_trans).flatten()\n",
    "test_y_trans_np = np.array(y_test).flatten()\n",
    "\n",
    "train_metrics_trans = calculate_metrics(train_y_trans_np, train_preds_trans_np)\n",
    "test_metrics_trans = calculate_metrics(test_y_trans_np, test_preds_trans_np)\n",
    "\n",
    "display_metrics(train_metrics_trans, \"Training Metrics:\")\n",
    "print()\n",
    "display_metrics(test_metrics_trans, \"Testing Metrics:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred vs True for training data\n",
      "0.8733, 1.0000\n",
      "0.8116, 1.0000\n",
      "0.6104, 1.0000\n",
      "0.7916, 1.0000\n",
      "0.8116, 1.0000\n",
      "0.8911, 1.0000\n",
      "0.8214, 1.0000\n",
      "0.8241, 1.0000\n",
      "0.8243, 1.0000\n",
      "0.8547, 1.0000\n",
      "\n",
      "Pred vs True for testing data\n",
      "0.4722, 0.7000\n",
      "0.4455, 0.7100\n",
      "0.5026, 0.4900\n",
      "0.3594, 0.2700\n",
      "0.3375, 0.3200\n",
      "0.4892, 0.4300\n",
      "0.3382, 0.3100\n",
      "0.3563, 0.3200\n",
      "0.5978, 0.7700\n",
      "0.3477, 0.3400\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred vs True for training data\")\n",
    "for i in range(10):\n",
    "    print(f\"{train_preds_trans_np[i]:.4f}, {train_labels[i]:.4f}\")\n",
    "print()\n",
    "print(\"Pred vs True for testing data\")\n",
    "for i in range(10):\n",
    "    print(f\"{test_preds_trans_np[i]:.4f}, {test_labels[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
